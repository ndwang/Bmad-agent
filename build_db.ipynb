{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from qdrant_db import QdrantDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean\n",
    "tex_files = glob.glob(\"bmad_doc/*.tex\")\n",
    "for file in tex_files:\n",
    "    filename_base = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Create the output file path\n",
    "    output_file = os.path.join(\"clean_bmad_doc\", f\"{filename_base}.txt\")\n",
    "    \n",
    "    # Process the file\n",
    "    clean.clean_latex_file(file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(text):\n",
    "    \"\"\"\n",
    "    Recursively chunks a document based on chapter-section-subsection hierarchy.\n",
    "    Preserves all original formatting including newlines.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The document text to chunk\n",
    "        \n",
    "    Returns:\n",
    "        dict: A nested dictionary representing the document structure\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    document = {\"content\": [], \"chapters\": {}}\n",
    "    \n",
    "    current_chapter = None\n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('###chapter '):\n",
    "            # Start a new chapter\n",
    "            chapter_title = line[len('###chapter '):].strip()\n",
    "            current_chapter = chapter_title\n",
    "            current_section = None\n",
    "            current_subsection = None\n",
    "            document[\"chapters\"][current_chapter] = {\n",
    "                \"title\": chapter_title,\n",
    "                \"content\": [],\n",
    "                \"sections\": {}\n",
    "            }\n",
    "        elif line.startswith('###section '):\n",
    "            # Start a new section within the current chapter\n",
    "            if current_chapter is None:\n",
    "                raise ValueError(f\"Section defined before any chapter at line {i+1}: {line}\")\n",
    "                \n",
    "            section_title = line[len('###section '):].strip()\n",
    "            current_section = section_title\n",
    "            current_subsection = None\n",
    "            document[\"chapters\"][current_chapter][\"sections\"][current_section] = {\n",
    "                \"title\": section_title,\n",
    "                \"content\": [],\n",
    "                \"subsections\": {}\n",
    "            }\n",
    "        elif line.startswith('###subsection '):\n",
    "            # Start a new subsection within the current section\n",
    "            if current_section is None:\n",
    "                raise ValueError(f\"Subsection defined before any section at line {i+1}: {line}\")\n",
    "                \n",
    "            subsection_title = line[len('###subsection '):].strip()\n",
    "            current_subsection = subsection_title\n",
    "            document[\"chapters\"][current_chapter][\"sections\"][current_section][\"subsections\"][current_subsection] = {\n",
    "                \"title\": subsection_title,\n",
    "                \"content\": []\n",
    "            }\n",
    "        else:\n",
    "            # Add content to the appropriate level (keeping each line separate)\n",
    "            if current_subsection is not None:\n",
    "                document[\"chapters\"][current_chapter][\"sections\"][current_section][\"subsections\"][current_subsection][\"content\"].append(line)\n",
    "            elif current_section is not None:\n",
    "                document[\"chapters\"][current_chapter][\"sections\"][current_section][\"content\"].append(line)\n",
    "            elif current_chapter is not None:\n",
    "                document[\"chapters\"][current_chapter][\"content\"].append(line)\n",
    "            else:\n",
    "                document[\"content\"].append(line)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def get_labeled_chunks(document):\n",
    "    \"\"\"\n",
    "    Convert the nested document structure into a flat list of labeled chunks.\n",
    "    Preserves original formatting including all newlines.\n",
    "    \n",
    "    Args:\n",
    "        document (dict): The nested document structure\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of (label, content) tuples\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Add document-level content if any\n",
    "    if document[\"content\"]:\n",
    "        chunks.append((\"document\", \"\\n\".join(document[\"content\"])))\n",
    "    \n",
    "    # Process chapters\n",
    "    for chapter_name, chapter in document[\"chapters\"].items():\n",
    "        if chapter[\"content\"]:\n",
    "            chunks.append((f\"chapter: {chapter_name}\", \"\\n\".join(chapter[\"content\"])))\n",
    "        \n",
    "        # Process sections\n",
    "        for section_name, section in chapter[\"sections\"].items():\n",
    "            if section[\"content\"]:\n",
    "                chunks.append((f\"chapter: {chapter_name} | section: {section_name}\", \"\\n\".join(section[\"content\"])))\n",
    "            \n",
    "            # Process subsections\n",
    "            for subsection_name, subsection in section[\"subsections\"].items():\n",
    "                if subsection[\"content\"]:\n",
    "                    chunks.append((\n",
    "                        f\"chapter: {chapter_name} | section: {section_name} | subsection: {subsection_name}\",\n",
    "                        \"\\n\".join(subsection[\"content\"])\n",
    "                    ))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_document_structure(document):\n",
    "    \"\"\"\n",
    "    Creates a tree-like visualization of the document structure.\n",
    "    \n",
    "    Args:\n",
    "        document (dict): The nested document structure from chunk_document()\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted string representation of the document structure\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Add document-level content\n",
    "    if document[\"content\"]:\n",
    "        result.append(\"Document\")\n",
    "        result.append(f\"   ({len(document['content'])} lines of content)\")\n",
    "    \n",
    "    # Process chapters\n",
    "    for chapter_name, chapter in document[\"chapters\"].items():\n",
    "        result.append(f\"Chapter: {chapter_name}\")\n",
    "        \n",
    "        # Add chapter content\n",
    "        if chapter[\"content\"]:\n",
    "            result.append(f\"   ({len(chapter['content'])} lines of content)\")\n",
    "        \n",
    "        # Process sections\n",
    "        for section_name, section in chapter[\"sections\"].items():\n",
    "            result.append(f\"   Section: {section_name}\")\n",
    "            \n",
    "            # Add section content\n",
    "            if section[\"content\"]:\n",
    "                result.append(f\"      ({len(section['content'])} lines of content)\")\n",
    "            \n",
    "            # Process subsections\n",
    "            for subsection_name, subsection in section[\"subsections\"].items():\n",
    "                result.append(f\"      Subsection: {subsection_name}\")\n",
    "                \n",
    "                # Add subsection content\n",
    "                if subsection[\"content\"]:\n",
    "                    result.append(f\"         ({len(subsection['content'])} lines of content)\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def visualize_document_with_stats(document):\n",
    "    \"\"\"\n",
    "    Visualizes the document structure with additional statistics.\n",
    "    \n",
    "    Args:\n",
    "        document (dict): The nested document structure from chunk_document()\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted string representation with statistics\n",
    "    \"\"\"\n",
    "    # Gather statistics\n",
    "    doc_lines = len(document[\"content\"])\n",
    "    chapter_count = len(document[\"chapters\"])\n",
    "    section_count = 0\n",
    "    subsection_count = 0\n",
    "    total_content_lines = doc_lines\n",
    "    \n",
    "    for chapter in document[\"chapters\"].values():\n",
    "        total_content_lines += len(chapter[\"content\"])\n",
    "        section_count += len(chapter[\"sections\"])\n",
    "        \n",
    "        for section in chapter[\"sections\"].values():\n",
    "            total_content_lines += len(section[\"content\"])\n",
    "            subsection_count += len(section[\"subsections\"])\n",
    "            \n",
    "            for subsection in section[\"subsections\"].values():\n",
    "                total_content_lines += len(subsection[\"content\"])\n",
    "    \n",
    "    # Generate visualization\n",
    "    result = [\n",
    "        \"DOCUMENT STRUCTURE SUMMARY\",\n",
    "        f\"Total content lines: {total_content_lines}\",\n",
    "        f\"Chapters: {chapter_count}\",\n",
    "        f\"Sections: {section_count}\",\n",
    "        f\"Subsections: {subsection_count}\",\n",
    "        \"\",\n",
    "        \"DETAILED STRUCTURE\",\n",
    "        \"=\" * 50\n",
    "    ]\n",
    "    \n",
    "    # Add regular visualization\n",
    "    result.append(visualize_document_structure(document))\n",
    "    \n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"clean_bmad_doc/elements.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    document_text = file.read()\n",
    "    \n",
    "# Parse the document\n",
    "document = chunk_document(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labeled chunks\n",
    "#chunks = get_labeled_chunks(doc_structure)\n",
    "print(visualize_document_structure(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_labeled_chunks(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Example: Load all PDFs from a directory\n",
    "loader = DirectoryLoader(\n",
    "    \"clean_bmad_doc\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'clean_bmad_doc/beam-init.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048,  # Adjust based on your needs\n",
    "    chunk_overlap=100,\n",
    "    separators=[\n",
    "        \n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\"\n",
    "    ]\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight local model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "vector_store.save_local(\"faiss_tao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vector_store = FAISS.load_local(\"faiss_clean\", embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})  # Fetch top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!--------------------\n",
      "\n",
      "track_start, track_end \n",
      "\"track_start\" and \"track_end\" are used when it is desired to only track the beam\n",
      "through part of the root lattice branch. \"track_start\" gives the starting element name or\n",
      "index. Tracking will start at the exit end of this element so the beam \\em will not be tracked\n",
      "through this element. The tracking will end at the exit end of the lattice element with name or\n",
      "index \"track_end\". The default, if \"track_start\" is not given, is to start at\n",
      "the beginning of the branch The default for \"track_end\" is the end of the root branch if the \n",
      "branch has an open geometry or beam tracking is beginning at the start of the branch. For a root\n",
      "branch with a closed geometry and with the beam starting in the middle, the tracking will wrap \n",
      "around from the branch end to the beginning of the branch and will end up just before the starting point.\n",
      "\n",
      "After initialization, the \"set beam_init\"  command can be used to set\n",
      "\"track_start\" and \"track_end\". Note: Deprecated names for \"track_start\" and\n",
      "\"track_end\" are \"track_start\" and \"track_end\" respectively.\n",
      "\n",
      "Tao will calculate a 6\\times6 \"beam sigma matrix\" from the lattice Twiss parameters and element\n",
      "transfer matrices. This can be useful for comparisons with the sigma matrix calculated from the\n",
      "distribution of a tracked beam  or for fast optimizations (the sigma matrix as\n",
      "calculated from the lattice can be done faster than tracking a beam). The \"\"show beam\n",
      "-lattice\"\" command (\\srefs:show.beam\") will display the lattice derived sigma matrix. For\n",
      "optimizations, the \"sigma\" data type (pg.~\\prefsigma) with \"data_source\" is set to \"lat\"\n",
      "can be used.\n",
      "\n",
      "!--------------------\n",
      "\n",
      "Beam apertures can be defined in the Bmad lattice file. Apertures can be defined in one of three\n",
      "ways. The most common is to set limit or aperture parameters for an element. Another possibility \n",
      "is to use a \"mask\" element (which can be used to define an aperture of arbitrary shape). The\n",
      "third possibility involves defining a continuous three-dimensional wall. This third possibility \n",
      "is only used with Runge-Kutta type tracking. \n",
      "\n",
      "To simplify things, the drawing of the beam aperture ignores any \"mask\" elements (since the\n",
      "geometry can be very complicated here) and ignores any three-dimensional walls (which are only\n",
      "used for Runge-Kutta type tracking). \\figf:aperture shows an example of a aperture drawing.\n",
      "\n",
      "To draw an aperture, a curve's \"data_source\" parameter must be set to \"\"aperture\"\" and the \"data_type\" parameter is set to one of\n",
      "  \"+x\"     ! Aperture in +X direction\n",
      "  \"-x\"     ! Aperture in -X direction\n",
      "  \"+y\"     ! Aperture in +Y direction\n",
      "  \"-y\"     ! Aperture in -Y direction\n",
      "The apertures in the +x and +y directions will have positive values and the apertures in the\n",
      "-x and -y directions will have negative values. Set the curve's \"y_axis_scale_factor\" to scale\n",
      "the aperture curve if needed.\n",
      "\n",
      "The following example will graphs the horizontal orbit along with the horizontal apertures.\n",
      "  &tao_template_plot\n",
      "    plot%name        = \"x_orbit\"\n",
      "    plot%x_axis_type = \"s\"\n",
      "    plot%n_graph     = 1\n",
      "  /\n",
      "\n",
      "  &tao_template_graph\n",
      "    graph%name    = \"x\"\n",
      "    graph_index   = 1\n",
      "\n",
      "    curve(1)%data_source  = \"aperture\"\n",
      "    curve(1)%data_type    = \"+x\"\n",
      "    curve(1)%draw_symbols = T\n",
      "    curve(1)%draw_line    = F\n",
      "    curve(1)%data_source  = \"aperture\"\n",
      "    curve(2)%data_type    = \"-x\"\n",
      "    curve(2)%draw_symbols = T\n",
      "    curve(2)%draw_line    = F\n",
      "    curve(3)%data_type = \"orbit.x\"\n",
      "  /\n",
      "Note: Aperture curves will ignore the \"curve%component\" parameter.\n",
      "\n",
      "  \\includegraphics[width=5in]dynamic-aperture.pdf\n",
      "\n",
      "!--------------------\n",
      "\n",
      "\\prefunstable.orbit   unstable.orbit                      \\begintabular@l   \n",
      "                                                                    Nonzero if particles are \n",
      "                                                                    lost in tracking\n",
      "  \\prefvelocity         velocity                            Normalized velocity v/c                 lat, beam  Yes \n",
      "  \\prefvelocity         velocity.x, .y, .z                  Normalized velocity component             lat, beam  Yes \n",
      "  \\prefwall             \\begintabular@l   \n",
      "                              wall.left_side, \n",
      "                              \\hspace4em .right_side\n",
      "  \\prefwire             wire.<angle>                        Wire scanner at <angle>                   beam       No\n",
      "\n",
      "!--------------------\n",
      "\n",
      "The are two types of tracking implemented in Tao: single particle tracking and many particle\n",
      "multi-bunch tracking.  Single particle tracking is just that, the tracking of a single particle\n",
      "through the lattice. Many particle multi-bunch tracking creates a Gaussian distribution of particles\n",
      "at the beginning of the lattice and tracks each particle through the lattice, including any\n",
      "wakefields.  Single particle tracking is used by default. The \"global%track_type\" parameter\n",
      ", which is set in the initialization file, is used to set the tracking.\n",
      "\n",
      "Particle spin tracking has also been set up for single particle and many particle tracking. See\n",
      "Sections~ and for details on setting up spin tracking.\n",
      "\n",
      "After each Tao command is processed, the lattice and \"merit\" function are recalculated and the\n",
      "plot window is regenerated. The merit function determines how well the \"model\" fits the measured\n",
      "data. See Chapter~\\refc:opti for more information on the merit function and its use by the\n",
      "optimizer.\n",
      "\n",
      "!--------------------\n",
      "\n",
      "If spin tracking is desired then \"beam_init%init_spin\" must be set to true.  \n",
      "\n",
      "The three random number generator parameters (\"%random_engine\", \"%random_gauss_converter\", and\n",
      "\"%random_sigma_cutoff\") used for initializing the beam are set in the \"tao_global_struct\"\n",
      ". They may, however, be overridden for beam particle generation by setting the\n",
      "corresponding parameters in the \"beam_init\" structure. That is, separate parameters may be setup\n",
      "for beam particle generation verses everything else.  These parameters are explained in\n",
      "Section~.\n",
      "\n",
      "     dump_at \n",
      "See documentation on the \"dump_file\" parameter below.\n",
      "\n",
      "     dump_file \n",
      "If the beam size is large or the number of elements at which the beam is to be saved at is large, it\n",
      "may be problematic to store all the beam particle position information in memory until the end of\n",
      "tracking. If this is the case, the beam particle position information can be written directly to a\n",
      "file during tracking (and not saved in memory) by setting \"dump_at\" to a list of elements at\n",
      "which the position information is to be saved and setting \"dump_file\" to the name of the data\n",
      "file. The data file should have an \".h5\" or \".hdf5\" suffix to save the data in HDF5\n",
      "format. Otherwise, an ASCII file will be produced. The syntax for \"dump_at\" is the same at\n",
      "\"saved_at\". Saving directly to a file using \"dump_at\" is separate from saving to memory using\n",
      "\"saved_at\". Example\n",
      "  &tao_beam_init\n",
      "    dump_at = \"marker::m* *34w*\" ! Save beam at all markers starting with \"m\"\n",
      "                                 !  and all elements with \"34w\" in their name. \n",
      "    dump_file = \"beam_dump.h5\"\n",
      "  /\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"beam tracking\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Check retrieved text\n",
    "for doc in results:\n",
    "    print('\\n!--------------------\\n')\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
